{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFW climate biomass widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install progressbar2\n",
    "#!pip install retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import progressbar\n",
    "from retrying import retry\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table with biomass density and total biomass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GADM 3.6 admin 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file('/Users/Ben/Downloads/gadm36_shp/gadm36.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gadm_ids = df[['GID_0', 'ID_0', 'NAME_0', 'ID_1', 'NAME_1', 'ID_2', 'NAME_2','GID_1','GID_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gadm_ids[gadm_ids['GID_2'] == 'AFG.2.1_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = gadm_ids[gadm_ids['GID_0']=='BRA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp[tmp['GID_1'] == 'BRA.2_1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df[df['GID_2'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{len(missing_df)/len(df) * 100:3.2f}% of rows are missing admin-2 id codes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gid_2(gid_2):\n",
    "    \"\"\"Return dict of iso (string), and admin_1 and admin_2 (ints) from gid_2 entry.\"\"\"\n",
    "    try:\n",
    "        iso, admin_1, tmp_admin_2 = gid_2.split('.')\n",
    "        admin_2 = tmp_admin_2.split('_')[0]\n",
    "        return {'iso':iso, 'admin_1':int(admin_1), 'admin_2':int(admin_2)}\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of GIDS to process\n",
    "all_areas = []\n",
    "for x in df['GID_2'].values:\n",
    "    tmp = process_gid_2(x)\n",
    "    if tmp:\n",
    "        all_areas.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gadm3.6 GID_2 data list\n",
    "with open(\"./data/gadm_36_gid2.json\", \"w\") as f:\n",
    "    for row in all_areas:\n",
    "        f.write(json.dumps(row) +'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all the codes for all areas I am going to de-allocate the memory of the df to save RAM\n",
    "df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin here if gadm 3.6 data file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing gadm-3.6 gid-2 file, restoring previous data! üç∫\n",
      "Loaded 338307 rows of data.\n"
     ]
    }
   ],
   "source": [
    "# Restore list of GID_2 data if the file exists\n",
    "gid_list = \"./data/gadm_36_gid2.json\"\n",
    "if os.path.exists(gid_list):\n",
    "    print(\"Found existing gadm-3.6 gid-2 file, restoring previous data! üç∫\")\n",
    "    with open(gid_list,\"r\") as f:\n",
    "        all_areas = []\n",
    "        for row in f.readlines():\n",
    "            all_areas.append(json.loads(row))\n",
    "    print(f'Loaded {len(all_areas)} rows of data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iso': 'AFG', 'admin_1': 1, 'admin_2': 1},\n",
       " {'iso': 'AFG', 'admin_1': 1, 'admin_2': 2},\n",
       " {'iso': 'AFG', 'admin_1': 1, 'admin_2': 3},\n",
       " {'iso': 'AFG', 'admin_1': 1, 'admin_2': 4},\n",
       " {'iso': 'AFG', 'admin_1': 1, 'admin_2': 5}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_areas[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API contains an endpoint for `whrc-biomass` to compute the total biomass and biomass density of a given municipality which uses geostore v2 endpoint for gadm geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use session to persist connection between requests (for speed-up) http://docs.python-requests.org/en/master/user/advanced/\n",
    "s = requests.Session() \n",
    "\n",
    "@retry(stop_max_attempt_number=5, wait_fixed=2000)\n",
    "def make_query(area):\n",
    "    try:\n",
    "        r = s.get(f\"https://production-api.globalforestwatch.org/v1/whrc-biomass/admin/{area['iso']}/{area['admin_1']}/{area['admin_2']}\")\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get('data').get('attributes')\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        print(f\"Failed on {area['iso']}/{area['admin_1']}/{area['admin_2']}\")\n",
    "        raise IOError(f\"EE failure: {r.status_code}\")\n",
    "\n",
    "                  \n",
    "def find_in_written_data(written_data, iso, admin_1, admin_2):\n",
    "    for row in written_data:\n",
    "        if row.get('iso') == iso and row.get('admin_1') == admin_1 and row.get('admin_2') == admin_2:\n",
    "            return True\n",
    "        else:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "                  \n",
    "def get_written_data(backup_file):\n",
    "    '''Create or restore data from a backup file e.g ./tmp_whrc_data.json '''\n",
    "    if os.path.exists(backup_file):\n",
    "        #print(\"Found existing file, restoring previous data! üç∫\")\n",
    "        written_data = []\n",
    "        with open(backup_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                written_data.append(json.loads(line))\n",
    "        return written_data\n",
    "    else:\n",
    "        #print(\"No previous data found, starting queries from scratch... üèÉ‚Äç‚ôÇÔ∏è\")    \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single thread process\n",
    "\n",
    "# %%time\n",
    "# with open(backup_file, \"a+\") as f:\n",
    "#     with progressbar.ProgressBar(max_value=len(all_areas)) as bar:\n",
    "#         for n, area in enumerate(all_areas[0:40]):\n",
    "#             bar.update(n)\n",
    "#             if not find_in_written_data(written_data, area.get('iso'), area.get('admin_1'), area.get('admin_2')):\n",
    "#                 # maybe we should try it several times if it fails....\n",
    "#                 tmp_data = make_query(area)\n",
    "#                 if tmp_data:\n",
    "#                     tmp_d = {**area, **tmp_data}\n",
    "#                     written_data.append(tmp_d)\n",
    "#                     f.write(json.dumps(tmp_d) +'\\n') # write a line to a temporary file incase the process fails and all data is lost\n",
    "#             else:\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gid_list(gid_list, backup_file=\"./tmp_whrc_data.json\"):\n",
    "    \"\"\"e.g. process_gid_list(all_areas[0:20])\"\"\"\n",
    "    written_data = get_written_data(backup_file)\n",
    "    with open(backup_file, \"a+\") as f:\n",
    "        with progressbar.ProgressBar(max_value=len(gid_list)) as bar:\n",
    "            for n, area in enumerate(gid_list):\n",
    "                bar.update(n)\n",
    "                #print(f\"Already processed area = {find_in_written_data(written_data, area.get('iso'), area.get('admin_1'), area.get('admin_2'))}\")\n",
    "                if not find_in_written_data(written_data, area.get('iso'), area.get('admin_1'), area.get('admin_2')):\n",
    "                    tmp_data = make_query(area)\n",
    "                    if tmp_data:\n",
    "                        tmp_d = {**area, **tmp_data}\n",
    "                        written_data.append(tmp_d)\n",
    "                        f.write(json.dumps(tmp_d) +'\\n') # write a line to a temporary file incase the process fails and all data is lost\n",
    "                else:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreadded requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 1000\n",
    "chunked_list = [all_areas[i:i + chunks] for i in range(0, len(all_areas), chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 1000) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Pool(100) as p:\n",
    "    p.map(process_gid_list, chunked_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_data = []\n",
    "with open(\"./tmp_whrc_data.json\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        check_data.append(json.loads(line))\n",
    "len(check_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the written data and create a final output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you need to load/restore the data from a tmp file (due to failure etc) you can do the following...\n",
    "written_data = []\n",
    "with open(\"./tmp_whrc_data.json\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        written_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final table needs row names of 'biomassdensity','gid_0','id_1','id_2','totalbiomass','areaHa'. Use rename function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(written_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admin_1</th>\n",
       "      <th>admin_2</th>\n",
       "      <th>areaHa</th>\n",
       "      <th>biomassDensity</th>\n",
       "      <th>iso</th>\n",
       "      <th>totalBiomass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>351695.388169</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>AFG</td>\n",
       "      <td>299.115355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>199722.098719</td>\n",
       "      <td>0.376596</td>\n",
       "      <td>AFG</td>\n",
       "      <td>75214.485758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>184545.374901</td>\n",
       "      <td>0.087080</td>\n",
       "      <td>AFG</td>\n",
       "      <td>16070.197273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>90486.036210</td>\n",
       "      <td>0.005928</td>\n",
       "      <td>AFG</td>\n",
       "      <td>536.407307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>297624.405024</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>AFG</td>\n",
       "      <td>3944.064107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admin_1  admin_2         areaHa  biomassDensity  iso  totalBiomass\n",
       "0        1        5  351695.388169        0.000850  AFG    299.115355\n",
       "1        1        6  199722.098719        0.376596  AFG  75214.485758\n",
       "2        1        9  184545.374901        0.087080  AFG  16070.197273\n",
       "3        1       10   90486.036210        0.005928  AFG    536.407307\n",
       "4        1        7  297624.405024        0.013252  AFG   3944.064107"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df.rename(index=str, columns={'admin_1':'id_1','admin_2':'id_2','biomassDensity':'biomassdensity','totalBiomass':'totalbiomass'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, save the file\n",
    "output_df.to_csv('./whrc_biomass.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
